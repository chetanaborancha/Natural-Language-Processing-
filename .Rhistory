##Plot the 100 most frequently occurring words.
set.seed(142)
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)
## On a PC, save the folder to your C: drive and use the following code chunk:
cname <- file.path("C:", "R text Mining", "texts")
cname
dir(cname)
## Load the R package for text mining and then load your texts into R.
library(tm)
docs <- Corpus(DirSource(cname))
summary(docs)
## Removing punctuation:
##  Your computer cannot actually read. Punctuation and other special characters only look like more words to your computer and R. Use the following to methods to remove them from your text
docs <- tm_map(docs, removePunctuation)
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
}
## Removing numbers:
docs <- tm_map(docs, removeNumbers)
## Converting to lowercase:
##   As before, we want a word to appear exactly the same every time it appears. We therefore change everything to lowercase.
docs <- tm_map(docs, tolower)
inspect(docs[2])
## Removing "stopwords" (common words) that usually have no analytic value.
## In every text, there are a lot of common, and uninteresting words (a, and, also, the, etc.). Such words are frequent by their nature, and will confound your analysis if they remain in the text.
# For a list of the stopwords, see:
# length(stopwords("english"))
# stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
## Removing particular words:
##  If you find that a particular word or two appear in the output, but are not of value to your particular analysis. You can remove them, specifically, from the text.
docs <- tm_map(docs, removeWords, c("department", "email"))
for (j in seq(docs))
{
docs[[j]] <- gsub("Peloponnesian War", "PW", docs[[j]])
docs[[j]] <- gsub("World War I", "WWI", docs[[j]])
docs[[j]] <- gsub("World War II", "WWII", docs[[j]])
}
## Removing common word endings (e.g., "ing", "es", "s")
## This is referred to as "stemming" documents.
## We stem the documents so that a word will be recognizable to the computer,
## whether or not it may have a variety of possible endings in the original text.
library(SnowballC)
## Stripping unnecesary whitespace from your documents:
##   The above preprocessing will leave the documents with a lot of "white space". White space is the result of all the left over spaces that were not removed along with the words that were deleted. The white space can, and should, be removed.
docs <- tm_map(docs, stripWhitespace)
dtm <- DocumentTermMatrix(docs)
dtm
## To inspect, you can use: inspect(dtm)
## This will, however, fill up your terminal quickly. So you may prefer to view a subset:
inspect(dtm[1:2, 1:20]) #view first 2 docs & first 20 terms - modify as you like
dim(dtm) #This will display the number of documents & terms (in that order)
## You'll also need a transpose of this matrix. Create it using:
tdm <- TermDocumentMatrix(docs)
tdm
## Organize terms by their frequency:
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
##Export the matrix to Excel:
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="dtm.csv")
## Have you ever seen an Excel file with tha tmany columns??
# Try this instead":
mt <- as.matrix(tdm)
dim(mt)
write.csv(mt, file="tdm.csv")
## Let's focus on just the interesting stuff.
#  Start by removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
## There are a lot of terms, so for now, just check out some of the most and least frequently occurring words.
freq[head(ord)]
freq[tail(ord)]
inspect(dtm)
## Check out the frequency of frequencies.
head(table(freq), 20)
## The resulting output is two rows of numbers.
## The top number is the frequency with which words appear
## and the bottom number reflects how many words appear that frequently.
## Here, considering only the 20 lowest word frequencies, we can see that
## 7407 terms appear only once and 2562 terms appear twice.
## There are also a lot of others that appear very infrequently:
tail(table(freq), 20)
## For a less, fine-grained look at term freqency we can view a table of the terms we selected when we removed sparse terms, above. (Look just under the word "Focus".)
freq <- colSums(as.matrix(dtms))
freq
## Or:
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, 14)
## An alternate view of term frequency:
##  This will identify all terms that appear frequently (in this case, 50 or more times).
findFreqTerms(dtm, lowfreq=50)   # Change "50" to whatever is most appropriate for your text data.
## another way to do this:
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
##Plot words that occur at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250)
## Humans are generally strong at visual analytics.
## That is part of the reason that these have become so popular. What follows are a variety of alternatives for constructing word clouds with your text.
## But first you will need to load the package that makes word clouds in R.
library(wordcloud)
##Plot words that occur at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250)
## Plot the 100 most frequently used words.
set.seed(142)
wordcloud(names(freq), freq, max.words=100)
##Plot words that occur at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250)
## Plot the 100 most frequently used words.
set.seed(142)
wordcloud(names(freq), freq, max.words=100)
##Add some color and plot words occurring at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
## Humans are generally strong at visual analytics.
## That is part of the reason that these have become so popular. What follows are a variety of alternatives for constructing word clouds with your text.
## But first you will need to load the package that makes word clouds in R.
library(wordcloud)
##Plot words that occur at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250)
## Plot the 100 most frequently used words.
set.seed(142)
wordcloud(names(freq), freq, max.words=100)
##Add some color and plot words occurring at least 250 times.
set.seed(142)
wordcloud(names(freq), freq, min.freq=250, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
##Plot the 100 most frequently occurring words.
set.seed(142)
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)
